Algorithm idea for male vs female multiple objective gradient boosting:
- Compute gradient (derivate using dsquareloss()) for both male dataset and female datasets independently
- Pass gradients into find_pareto_descent() to get descent direction (psuedo-residuals)
- Build tree to predict psuedo-residuals

Concerns:
- What does the LIBSVM encoding mean?
- Is the dataset the same as the UCI dataset?
    - No: many fewer features in the LIBSVM dataset
- Code takes a very long time to run (>5 minutes) on UCI preprocessed data
    - Large number of features and data points
    - LIBSVM dataset has many fewer features and data points

------------------------------------ CURRENTLY -------------------------------------------------------
#  In original XGBoost code's python package, core.py
pred = self.predict(dtrain)
grad, hess = fobj(pred, dtrain)
self.boost(dtrain, grad, hess)

------------------------------------ DESIRED ---------------------------------------------------------
# dtrain1 is training data with first set of labels (first objective)
pred1 = self.predict(dtrain1)
grad1, hess1 = fobj(pred1, dtrain1)

# dtrain2 is training data with second set of labels (second objective)
pred2 = self.predict(dtrain2)
grad2, hess2 = fobj(pred2, dtrain2)

# Apply pareto to gradients (first deriv    ative of objective function)
grad_mat = np.hstack((grad1.reshape(-1, 1), grad2.reshape(-1, 1)))
grad_pareto = find_pareto_descent(grad_mat)

# Apply pareto to hessians (second derivative of objective function)
hess_mat = np.hstack((hess2.reshape(-1, 1), hess2.reshape(-1, 1)))
hess_pareto = find_pareto_descent(hess_mat)

# What will the original dtrain look like?
# Would the original dtrain dataset include each label (in this case, label1 and label2)?
# How would this be passed into the fit() function? As a 2D y array?
self.boost(dtrain, grad_pareto, hess_pareto)

------------------------------------ NOTES -----------------------------------------------------------
- Make 2 global DMatrices, dtrain1 and dtrain2
    dtrain = xgb.DMatrix(x_train, label=y_train)
- Use the above 'DESIRED' code
- This way, won't have to worry about making sure arguments line up since the dtrain arguments in
  update(self, dtrain, iteration, fobj=None) will not be used
- To test, call fit(X, y) with bogus y (all 1s or something)

- STILL NEED TO FIX MODEL INITIALIZATION??
    - What are the model's initial predictions?
        - Does this matter?
    - Where are initial predictions set?
        - Potentially: change y argument passed into fit() method to mock up the average prediction

------------------------------------------------------------------------------------------------------
Having difficulty with PyCharm. Instead, could make changes directly to pip-installed xgboost package
using atom:

Location of pip-installed xgboost package:
    /Users/Ashtekar15/anaconda3/lib/python3.6/site-packages/xgboost

------------------------------------------------------------------------------------------------------
Instead of making changes to pip-installed xgboost, make changes inside venv
    (Use shift+command+g to search)
    /Users/Ashtekar15/anaconda3/envs/ModXGBoost/lib/python3.7/site-packages/xgboost

To run test script (in /Users/Ashtekar15/Desktop/Thesis):
    conda activate ModXGBoost
    python xgb_test.py

------------------------------------------------------------------------------------------------------

In sklearn.py, class XGBModel(XGBModelBase):
   Note
   ----
   A custom objective function can be provided for the ``objective``
   parameter. In this case, it should have the signature
   ``objective(y_true, y_pred) -> grad, hess``:

------------------------------------------------------------------------------------------------------
Model is currently working for adult dataset, to do next:
- Clean up code
    - Put most code in core.py/sklearn.py, not in the xgb_test.py script
        - Move the fobj function to one of these files (?)
            - Put fobj() in sklearn.py, as __init__() argument for
              class XGBModel(XGBModelBase)
        - May need to modify a different file

        DONE

    - Inlcude a loop to iterate over all objectives (dtrain1, dtrain2, etc.)
        - Also use a loop to create DMatrix for each set of labels (all with
          the same features)
        - Move this code into a function (method of Booster (?)) in core.py
         rather than sitting out

         DONE

- Look at demos (xgboost/demo in original repo)
    - Test on Higgs dataset
    - Confirm that labels passed into fit() do not matter
- Set up version control
    - New repo (?)
    - Folder within existing repo (?)

So far, have modified files:
    core.py
    sklearn.py

------------------------------------------------------------------------------------------------------
Model currently working on both adult and higgs datasets

To do:
- Confirm that Higgs is a regression problem
  ANSWER: Ultimately a multiclass classification problem, however, the
          included prediction script takes in continuous predictions and
          thresholds them in order to make categorical predictions.

- Try to match 3.6 score on Kaggle
    - Use train/test split, don't submit predictions (?)
    - Run with MSE objective function, 120 trees
      ANSWER: Results are close (3.41 vs 3.6 MG vs XG) but not equal. Attribute
              difference to "objective='binary:logitraw'" in original XG code

- Compare results of traditional XGBoost to MGBoost on a simple regression
  problem (ex: linear regression)
  ANSWER: Results exactly the same

- Clarify questions to ask

------------------------------------------------------------------------------------------------------
New gituhb repo:
- xgboost/
- other/
    - test_scripts/
    - test_data/
- notes.txt
- README.md

------------------------------------------------------------------------------------------------------
To do before 11-4 meeting:
- Read (skim) thru Microsoft paper
- Create updated github repo
    - Share with Mehrdad
- Make changes to __init__ of MGBoost (from xgboost source code)

------------------------------------------------------------------------------------------------------
To do 11-17:
- Write function for creating features from input data
    - Make function usable with Microsoft bing dataset
- Test performance of various parameter configs
- Email function script, jupyter notebook with analysis
    - Remind Mehrdad to approve thesis in email
    - Mention questions/ideas regarding feature generation
    - Ask about meeting this coming week
- Re-read Chris Burges lambda ranking paper
- Think about research questions
    - Why does the inclusion of the A - B feature help?
    - Adding idea of lambda to xgboost ranking:
        - By modifying xgboost algorithm -- straightforward? How to?
        - Without modifying xgboost algorithm -- how?

Microsoft learning-to-rank datasets:
https://www.microsoft.com/en-us/research/project/mslr/

Concerns:
- Features will be too big to fit in memory (8GB RAM in laptop)
- Don't know which column in sample.tsv corresponds to score
- (?)

------------------------------------------------------------------------------------------------------
To do 11-19:
- Write function
- Do experiments
- Skim thru Chris Burges paper
- Think about features/performance
- Email Mehrdad results
    - MENTION THESIS APPROVAL

------------------------------------------------------------------------------------------------------
To do 11-21:
- Look at unbiased lambdamart code
- Skim thru Chris Burges paper
- Think about features/performance
- Review your summary

------------------------------------------------------------------------------------------------------
Notes 1-8:
- Can easily load train/val/test split data from Yahoo LETOR dataset using functions in unbiased
  lambdamart

  Data explanation: https://github.com/QingyaoAi/Unbiased-Learning-to-Rank-with-Unbiased-Propensity-Estimation
  Script to load data: https://github.com/QingyaoAi/Unbiased-Learning-to-Rank-with-Unbiased-Propensity-Estimation/blob/master/Unbiased_LTR/data_utils.py

- Generated features are still much too large to fit in memory (100+ GB)
- Options:
    - Train/test split with MSFT dataset
        - Compare delta_features=True vs delta_features=False
            - Report results
    - Rewrite feature generation functions for Yahoo data
        - Make cleaner than MSFT scripts
    - Fix MGBoost to only load data once
        - Modify __init__
    - Email Mehrdad
        - Mention problem/using cluster
        - Send paper https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/suml10_submission_20.pdf
        - Ask to focus on MGBoost wrt KDD
            - Mention fixing MGBoost
        - Ask about third project
    - Review theory
        - Chris Burges LTR paper
        - MGBoost problem formulation
        - Read nature paper


------------------------------------------------------------------------------------------------------
Notes 1-9:
- First test: delta_features leads to worsened performance
- Moving forward:
    - Run more times with different random seeds
        - At least 5 (?) more times
    - Modify XGBoost hyperparameters
        - Maybe current model is too complex
            - Fewer trees
            - Shallower max depth
    - Think of explanation
        - Curse of dimensionality
        - Model complexity
            - Overfitting with more features
        - Too little data relative to number of features
    - Try using ONLY delta_features as features
















.
