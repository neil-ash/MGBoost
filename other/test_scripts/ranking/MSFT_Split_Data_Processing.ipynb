{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data Processing\n",
    "\n",
    "Testing the performance of various feature configurations when using DeltaMART with the MSLR-WEB10K dataset. \n",
    "\n",
    "https://www.microsoft.com/en-us/research/project/mslr/\n",
    "\n",
    "Only using a small subset of queries (10 at the moment) given that the notebook is run locally on a laptop with 8GB RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.special import expit  # Logistic function\n",
    "from rank_metrics import ndcg_at_k, mean_average_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataframe(filepath, n_queries=30, seed=1):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    # For reproducible results from randomly selecting queries\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    df = pd.read_csv(filepath,\n",
    "                     sep=' ',\n",
    "                     header=None)\n",
    "    \n",
    "    # Remove last column of NaN\n",
    "    df = df.iloc[:, :-1]\n",
    "    \n",
    "    # First column: hand-labeled score, second column: query id\n",
    "    df = df.rename(columns={0: 'label', 1: 'query_id'})\n",
    "    \n",
    "    # Get random sample of queries\n",
    "    qids = df.query_id.unique()\n",
    "    qids = np.random.choice(qids, size=n_queries)\n",
    "    \n",
    "    # Only save dataframe with queries of interest\n",
    "    df = df[df.query_id.isin(qids)]\n",
    "    \n",
    "    # Save hand-labels\n",
    "    labels = df.label\n",
    "\n",
    "    # Use regex to get number after colon for every column other than label\n",
    "    features = df.iloc[:, 1:].applymap(lambda x: float(re.findall(r':(.*)', x)[0]))\n",
    "\n",
    "    # Put features and labels in same dataframe\n",
    "    df = features\n",
    "    df['label'] = labels\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df, repeat_importance, two_sided, delta_features):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    n_rows = 0\n",
    "    max_diff = 4\n",
    "    n_features = 136\n",
    "    \n",
    "    # Find max possible number of rows: n_queries * (n_urls_per_query ^ 2) * max_repeat_factor\n",
    "    for qid in df.query_id.unique():\n",
    "        urls_per_query = df[df.query_id == qid].shape[0]\n",
    "        \n",
    "        # If not repeating importance, then every query-URL pair only appears once\n",
    "        if repeat_importance:\n",
    "            n_rows += (urls_per_query ** 2) * max_diff\n",
    "        else:\n",
    "            n_rows += (urls_per_query ** 2)\n",
    "    \n",
    "    # Add extra set of columns if delta_features, + 4 for i, j, query_id, label\n",
    "    if delta_features:\n",
    "        n_columns = n_features * 3 + 4\n",
    "    else:\n",
    "        n_columns = n_features * 2 + 4\n",
    "    \n",
    "    # Create array to fill in later (faster)\n",
    "    features = np.full(shape=(n_rows, n_columns), fill_value=np.nan)\n",
    "    idx = 0\n",
    "    \n",
    "    # Compare each URL for a given query\n",
    "    for progress, qid in enumerate(df.query_id.unique()):\n",
    "        \n",
    "        # tdf: temporary dataframe, m: number of URLs in tdf\n",
    "        tdf = df[df.query_id == qid]\n",
    "        m = tdf.shape[0]\n",
    "        \n",
    "        # First URL\n",
    "        for i in range(m):\n",
    "            \n",
    "            # Two sided: feature (a, b) will be repeated later as feature (b, a)\n",
    "            if two_sided:\n",
    "                start_j = 0\n",
    "            else:\n",
    "                start_j = i\n",
    "            \n",
    "            # Second URL\n",
    "            for j in range(start_j, m):\n",
    "                \n",
    "                label_diff = tdf.label.iloc[i] - tdf.label.iloc[j]\n",
    "                \n",
    "                # Repeat importance: duplicate row |label_diff| times\n",
    "                if repeat_importance:\n",
    "                    end_k = int(abs(label_diff)) + 1\n",
    "                else:\n",
    "                    end_k = 1\n",
    "                    \n",
    "                for k in range(end_k):\n",
    "                    \n",
    "                    # Delta features: for feature (a, b), represent as (a, b, a-b)\n",
    "                    # Format: (i, j, query_id, URLi, URLj, URLi-URLj (?), label_diff)\n",
    "                    if delta_features:\n",
    "                        new_row = np.hstack((i,\n",
    "                                             j,\n",
    "                                             qid,\n",
    "                                             tdf.iloc[i, 1:-1], \n",
    "                                             tdf.iloc[j, 1:-1], \n",
    "                                             tdf.iloc[i, 1:-1] - tdf.iloc[j, 1:-1],  \n",
    "                                             label_diff))\n",
    "                    else:\n",
    "                            new_row = np.hstack((i,\n",
    "                                                 j,\n",
    "                                                 qid,\n",
    "                                                 tdf.iloc[i, 1:-1], \n",
    "                                                 tdf.iloc[j, 1:-1],  \n",
    "                                                 label_diff))\n",
    "                        \n",
    "                    features[idx] = new_row\n",
    "                    idx += 1\n",
    "\n",
    "#         print(progress)\n",
    "    \n",
    "    # Originally allocated array is likely too large, only save relevant rows\n",
    "    features = features[~np.isnan(features[:, 0])]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train_feat, test_feat, labels):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    \"\"\" TRAIN \"\"\"\n",
    "    # Features does not include i, j, does includes query_id\n",
    "    X_train = train_feat[:, 2:-1]\n",
    "    y_train = train_feat[:, -1]\n",
    "\n",
    "    # Same parameters for all calls to ensure consistency\n",
    "    xgbr = XGBRegressor(max_depth=6, \n",
    "                        learning_rate=0.1,\n",
    "                        n_estimators=100, # Change to make faster OR more powerful (?)\n",
    "                        objective='reg:squarederror')\n",
    "    \n",
    "    xgbr.fit(X_train, y_train)\n",
    "\n",
    "    print('Model fitted')\n",
    "\n",
    "    \"\"\" TEST \"\"\"\n",
    "    # Want to make predictions on every URL pair within a query, for all queries\n",
    "    # Avoid predicting on rows that were repeated above\n",
    "    # Combo of i, j query_id ensures that unique will work to prevent repeated rows\n",
    "    feat_unique = np.unique(test_feat, axis=0)\n",
    "    X_unique = feat_unique[:, 2:-1]\n",
    "    y_pred = xgbr.predict(X_unique)\n",
    "\n",
    "    # Record results over all queries\n",
    "    MAP = 0\n",
    "    NDCG1, NDCG3, NDCG5, NDCG10, NDCGM = 0, 0, 0, 0, 0\n",
    "    \n",
    "    # For each query, make a prediction array (scores)\n",
    "    for qid in np.unique(X_unique[:, 0]):\n",
    "\n",
    "        # m will be the number of URLs per given query ID\n",
    "        m = int(np.sqrt(np.sum(X_unique[:, 0] == qid)))\n",
    "\n",
    "        # Save y_pred only for query of interest as y_pq, reshape in order to sum across rows\n",
    "        # Note that the default order='C' in reshape is fine (row-major)\n",
    "        # Setting order='F' will result in roughly the same result, just reversed since the \n",
    "        # learned labels correspond to (URLi - URLj)\n",
    "        y_pq = y_pred[X_unique[:, 0] == qid]\n",
    "        y_pq = y_pq.reshape(m, m, order='C')\n",
    "\n",
    "        # Apply logistic function\n",
    "        y_pq = expit(y_pq)\n",
    "\n",
    "        # Sum across rows to get 'power' of each individual training example\n",
    "        # Get order using the scores as indices\n",
    "        scores = np.sum(y_pq, axis=0)\n",
    "        order = np.argsort(scores)\n",
    "\n",
    "        # Apply order to original labels\n",
    "        y_orig = labels[labels.query_id == qid].label.values\n",
    "        r = y_orig[order]\n",
    "\n",
    "        # Get results\n",
    "        m_a_p = mean_average_precision([r])\n",
    "        n1, n3, n5, n10, nm = ndcg_at_k(r=r, k=1),ndcg_at_k(r=r, k=3), ndcg_at_k(r=r, k=5), ndcg_at_k(r=r, k=10), ndcg_at_k(r=r, k=m)\n",
    "\n",
    "        # Update overall results\n",
    "        MAP += m_a_p\n",
    "        NDCG1 += n1\n",
    "        NDCG3 += n3\n",
    "        NDCG5 += n5\n",
    "        NDCG10 += n10\n",
    "        NDCGM += nm\n",
    "        \n",
    "        # Results for query\n",
    "        print('Query %d, m=%d:' % (qid, m))\n",
    "        print('\\tNDCG@1:  %.4f' % n1)\n",
    "        print('\\tNDCG@3: %.4f' % n3)\n",
    "        print('\\tNDCG@5: %.4f' % n5)\n",
    "        print('\\tNDCG@10: %.4f' % n10)\n",
    "        print('\\tNDCG@m:  %.4f' % nm)\n",
    "\n",
    "    # Results over all queries\n",
    "    print('\\nOverall:')\n",
    "    print('\\tMAP:     %.4f' % (MAP / 10))\n",
    "    print('\\tNDCG@1:  %.4f' % (NDCG1 / 10))\n",
    "    print('\\tNDCG@3:  %.4f' % (NDCG3 / 10))\n",
    "    print('\\tNDCG@5:  %.4f' % (NDCG5 / 10))\n",
    "    print('\\tNDCG@10: %.4f' % (NDCG10 / 10))\n",
    "    print('\\tNDCG@m:  %.4f' % (NDCGM / 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SETUP BETWEEN TRIALS HERE \"\"\"\n",
    "my_seed = 5\n",
    "delta_features = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_df = generate_dataframe('/Users/Ashtekar15/Desktop/Thesis/MGBoost/other/test_data/ranking/MSLR-WEB10K/Fold1/vali.txt', \n",
    "                           n_queries=10, \n",
    "                           seed=my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Sets shape of train/test features during initilization\n",
    "if delta_features:\n",
    "    n = 412\n",
    "else:\n",
    "    n = 276\n",
    "\n",
    "# Initlize to then fill in\n",
    "train = np.full(shape=(1, n), fill_value=np.nan)\n",
    "test = np.full(shape=(1, n), fill_value=np.nan)\n",
    "labels = pd.DataFrame()\n",
    "\n",
    "for progress, qid in enumerate(np.unique(my_df.query_id)):\n",
    "    \n",
    "    # Split train/test -> 80/20\n",
    "    train_df, test_df = train_test_split(my_df[my_df.query_id == qid], test_size=0.2)\n",
    "    \n",
    "    # Save test labels (in order to evaluate final rank)\n",
    "    labels = labels.append(test_df[['query_id', 'label']], ignore_index=True)\n",
    "    \n",
    "    # Make train/test features for a given query, no need to repeat_importance for testing\n",
    "    train_f = generate_features(train_df, repeat_importance=True, two_sided=True, delta_features=delta_features)\n",
    "    test_f = generate_features(test_df, repeat_importance=False, two_sided=True, delta_features=delta_features)\n",
    "    \n",
    "    # Save train/test features\n",
    "    train = np.vstack((train, train_f))\n",
    "    test = np.vstack((test, test_f))\n",
    "    \n",
    "    print(progress)\n",
    "\n",
    "# Ignore first NaN row\n",
    "train = train[1:, :]\n",
    "test = test[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ashtekar15/anaconda3/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitted\n",
      "Query 1105, m=20:\n",
      "\tNDCG@1:  0.5000\n",
      "\tNDCG@3: 0.8100\n",
      "\tNDCG@5: 0.7759\n",
      "\tNDCG@10: 0.8260\n",
      "\tNDCG@m:  0.8957\n",
      "Query 3100, m=20:\n",
      "\tNDCG@1:  0.5000\n",
      "\tNDCG@3: 0.7246\n",
      "\tNDCG@5: 0.7808\n",
      "\tNDCG@10: 0.7920\n",
      "\tNDCG@m:  0.8821\n",
      "Query 11110, m=24:\n",
      "\tNDCG@1:  0.0000\n",
      "\tNDCG@3: 0.0000\n",
      "\tNDCG@5: 0.0000\n",
      "\tNDCG@10: 0.0000\n",
      "\tNDCG@m:  0.0000\n",
      "Query 13015, m=28:\n",
      "\tNDCG@1:  0.0000\n",
      "\tNDCG@3: 0.4319\n",
      "\tNDCG@5: 0.5269\n",
      "\tNDCG@10: 0.4475\n",
      "\tNDCG@m:  0.7235\n",
      "Query 14980, m=39:\n",
      "\tNDCG@1:  0.6667\n",
      "\tNDCG@3: 0.8403\n",
      "\tNDCG@5: 0.8153\n",
      "\tNDCG@10: 0.8198\n",
      "\tNDCG@m:  0.9034\n",
      "Query 15490, m=11:\n",
      "\tNDCG@1:  1.0000\n",
      "\tNDCG@3: 1.0000\n",
      "\tNDCG@5: 1.0000\n",
      "\tNDCG@10: 1.0000\n",
      "\tNDCG@m:  1.0000\n",
      "Query 17140, m=24:\n",
      "\tNDCG@1:  1.0000\n",
      "\tNDCG@3: 0.6806\n",
      "\tNDCG@5: 0.7538\n",
      "\tNDCG@10: 0.7696\n",
      "\tNDCG@m:  0.8505\n",
      "Query 21370, m=33:\n",
      "\tNDCG@1:  1.0000\n",
      "\tNDCG@3: 0.5508\n",
      "\tNDCG@5: 0.5329\n",
      "\tNDCG@10: 0.5254\n",
      "\tNDCG@m:  0.7259\n",
      "Query 25885, m=1:\n",
      "\tNDCG@1:  0.0000\n",
      "\tNDCG@3: 0.0000\n",
      "\tNDCG@5: 0.0000\n",
      "\tNDCG@10: 0.0000\n",
      "\tNDCG@m:  0.0000\n",
      "Query 26515, m=23:\n",
      "\tNDCG@1:  0.5000\n",
      "\tNDCG@3: 0.3100\n",
      "\tNDCG@5: 0.3828\n",
      "\tNDCG@10: 0.4678\n",
      "\tNDCG@m:  0.6907\n",
      "\n",
      "Overall:\n",
      "\tMAP:     0.6275\n",
      "\tNDCG@1:  0.5167\n",
      "\tNDCG@3:  0.5348\n",
      "\tNDCG@5:  0.5568\n",
      "\tNDCG@10: 0.5648\n",
      "\tNDCG@m:  0.6672\n"
     ]
    }
   ],
   "source": [
    "# build_model(train, test, labels)\n",
    "\n",
    "train_feat, test_feat, labels = train, test, labels\n",
    "\n",
    "\"\"\" TRAIN \"\"\"\n",
    "# Features does not include i, j, does includes query_id\n",
    "X_train = train_feat[:, 2:-1]\n",
    "y_train = train_feat[:, -1]\n",
    "\n",
    "# Same parameters for all calls to ensure consistency\n",
    "xgbr = XGBRegressor(max_depth=6, \n",
    "                    learning_rate=0.1,\n",
    "                    n_estimators=100, # Change to make faster OR more powerful (?)\n",
    "                    objective='reg:squarederror')\n",
    "\n",
    "xgbr.fit(X_train, y_train)\n",
    "\n",
    "print('Model fitted')\n",
    "\n",
    "\"\"\" TEST \"\"\"\n",
    "# Want to make predictions on every URL pair within a query, for all queries\n",
    "# Avoid predicting on rows that were repeated above\n",
    "# Combo of i, j query_id ensures that unique will work to prevent repeated rows\n",
    "feat_unique = np.unique(test_feat, axis=0)\n",
    "X_unique = feat_unique[:, 2:-1]\n",
    "y_pred = xgbr.predict(X_unique)\n",
    "\n",
    "# Record results over all queries\n",
    "MAP = 0\n",
    "NDCG1, NDCG3, NDCG5, NDCG10, NDCGM = 0, 0, 0, 0, 0\n",
    "\n",
    "# Save rankings (to visually compare)\n",
    "r_ls = []\n",
    "\n",
    "# For each query, make a prediction array (scores)\n",
    "for qid in np.unique(X_unique[:, 0]):\n",
    "\n",
    "    # m will be the number of URLs per given query ID\n",
    "    m = int(np.sqrt(np.sum(X_unique[:, 0] == qid)))\n",
    "\n",
    "    # Save y_pred only for query of interest as y_pq, reshape in order to sum across rows\n",
    "    # Note that the default order='C' in reshape is fine (row-major)\n",
    "    # Setting order='F' will result in roughly the same result, just reversed since the \n",
    "    # learned labels correspond to (URLi - URLj)\n",
    "    y_pq = y_pred[X_unique[:, 0] == qid]\n",
    "    y_pq = y_pq.reshape(m, m, order='C')\n",
    "\n",
    "    # Apply logistic function\n",
    "    y_pq = expit(y_pq)\n",
    "\n",
    "    # Sum across rows to get 'power' of each individual training example\n",
    "    # Get order using the scores as indices\n",
    "    scores = np.sum(y_pq, axis=0)\n",
    "    order = np.argsort(scores)\n",
    "\n",
    "    # Apply order to original labels\n",
    "    y_orig = labels[labels.query_id == qid].label.values\n",
    "    r = y_orig[order]\n",
    "    \n",
    "    # Save ranking\n",
    "    r_ls.append(r)\n",
    "\n",
    "    # Get results\n",
    "    m_a_p = mean_average_precision([r])\n",
    "    n1, n3, n5, n10, nm = ndcg_at_k(r=r, k=1),ndcg_at_k(r=r, k=3), ndcg_at_k(r=r, k=5), ndcg_at_k(r=r, k=10), ndcg_at_k(r=r, k=m)\n",
    "\n",
    "    # Update overall results\n",
    "    MAP += m_a_p\n",
    "    NDCG1 += n1\n",
    "    NDCG3 += n3\n",
    "    NDCG5 += n5\n",
    "    NDCG10 += n10\n",
    "    NDCGM += nm\n",
    "\n",
    "    # Results for query\n",
    "    print('Query %d, m=%d:' % (qid, m))\n",
    "    print('\\tNDCG@1:  %.4f' % n1)\n",
    "    print('\\tNDCG@3: %.4f' % n3)\n",
    "    print('\\tNDCG@5: %.4f' % n5)\n",
    "    print('\\tNDCG@10: %.4f' % n10)\n",
    "    print('\\tNDCG@m:  %.4f' % nm)\n",
    "\n",
    "# Results over all queries\n",
    "print('\\nOverall:')\n",
    "print('\\tMAP:     %.4f' % (MAP / 10))\n",
    "print('\\tNDCG@1:  %.4f' % (NDCG1 / 10))\n",
    "print('\\tNDCG@3:  %.4f' % (NDCG3 / 10))\n",
    "print('\\tNDCG@5:  %.4f' % (NDCG5 / 10))\n",
    "print('\\tNDCG@10: %.4f' % (NDCG10 / 10))\n",
    "print('\\tNDCG@m:  %.4f' % (NDCGM / 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "With delta_features terms False/True listed below (copy/pasted from output):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random seed = 1\n",
    "\n",
    "False\n",
    "Overall:\n",
    "\tMAP:     0.7628\n",
    "\tNDCG@1:  0.7000\n",
    "\tNDCG@3:  0.7175\n",
    "\tNDCG@5:  0.7215\n",
    "\tNDCG@10: 0.7233\n",
    "\tNDCG@m:  0.8196\n",
    "    \n",
    "True\n",
    "Overall:\n",
    "\tMAP:     0.7307\n",
    "\tNDCG@1:  0.5500\n",
    "\tNDCG@3:  0.5895\n",
    "\tNDCG@5:  0.6202\n",
    "\tNDCG@10: 0.6733\n",
    "\tNDCG@m:  0.7592\n",
    "    \n",
    "------------------------------------\n",
    "\n",
    "Random seed = 2  \n",
    "\n",
    "False\n",
    "Overall:\n",
    "\tMAP:     0.6444\n",
    "\tNDCG@1:  0.3167\n",
    "\tNDCG@3:  0.5901\n",
    "\tNDCG@5:  0.6254\n",
    "\tNDCG@10: 0.6445\n",
    "\tNDCG@m:  0.7481\n",
    "    \n",
    "True\n",
    "Overall:\n",
    "\tMAP:     0.5685\n",
    "\tNDCG@1:  0.4500\n",
    "\tNDCG@3:  0.4447\n",
    "\tNDCG@5:  0.4560\n",
    "\tNDCG@10: 0.5452\n",
    "\tNDCG@m:  0.6598\n",
    "    \n",
    "------------------------------------\n",
    "\n",
    "Random seed = 3  \n",
    "\n",
    "False\n",
    "Overall:\n",
    "\tMAP:     0.5472\n",
    "\tNDCG@1:  0.6167\n",
    "\tNDCG@3:  0.5459\n",
    "\tNDCG@5:  0.5630\n",
    "\tNDCG@10: 0.5696\n",
    "\tNDCG@m:  0.6740\n",
    "\n",
    "True\n",
    "Overall:\n",
    "\tMAP:     0.4850\n",
    "\tNDCG@1:  0.4333\n",
    "\tNDCG@3:  0.4269\n",
    "\tNDCG@5:  0.4888\n",
    "\tNDCG@10: 0.5237\n",
    "\tNDCG@m:  0.6299\n",
    "    \n",
    "------------------------------------\n",
    "\n",
    "Random seed = 4  \n",
    "\n",
    "False\n",
    "Overall:\n",
    "\tMAP:     0.5962\n",
    "\tNDCG@1:  0.5167\n",
    "\tNDCG@3:  0.4563\n",
    "\tNDCG@5:  0.4947\n",
    "\tNDCG@10: 0.5267\n",
    "\tNDCG@m:  0.6297\n",
    "\n",
    "True:\n",
    "Overall:\n",
    "\tMAP:     0.6074\n",
    "\tNDCG@1:  0.5500\n",
    "\tNDCG@3:  0.5209\n",
    "\tNDCG@5:  0.5116\n",
    "\tNDCG@10: 0.5391\n",
    "\tNDCG@m:  0.6439\n",
    "        \n",
    "------------------------------------\n",
    "\n",
    "Random seed = 5 \n",
    "\n",
    "False\n",
    "Overall:\n",
    "\tMAP:     0.6079\n",
    "\tNDCG@1:  0.5333\n",
    "\tNDCG@3:  0.5449\n",
    "\tNDCG@5:  0.5460\n",
    "\tNDCG@10: 0.5638\n",
    "\tNDCG@m:  0.6654\n",
    "    \n",
    "Overall:\n",
    "\tMAP:     0.6275\n",
    "\tNDCG@1:  0.5167\n",
    "\tNDCG@3:  0.5348\n",
    "\tNDCG@5:  0.5568\n",
    "\tNDCG@10: 0.5648\n",
    "\tNDCG@m:  0.6672"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
