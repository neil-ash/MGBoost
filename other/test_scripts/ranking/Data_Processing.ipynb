{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.special import expit  # Logistic function\n",
    "from rank_metrics import ndcg_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataframe(filepath, n_queries=30, seed=1):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    # For reproducible results from randomly selecting queries\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    df = pd.read_csv(filepath,\n",
    "                       sep=' ',\n",
    "                       header=None)\n",
    "    \n",
    "    # Remove last column of NaN\n",
    "    df = df.iloc[:, :-1]\n",
    "    \n",
    "    # First column: hand-labeled score, second column: query id\n",
    "    df = df.rename(columns={0: 'label', 1: 'query_id'})\n",
    "    \n",
    "    # Get random sample of queries\n",
    "    qids = df.query_id.unique()\n",
    "    qids = np.random.choice(qids, size=n_queries)\n",
    "    \n",
    "    # Only save dataframe with queries of interest\n",
    "    df = df[df.query_id.isin(qids)]\n",
    "    \n",
    "    # Save hand-labels\n",
    "    labels = df.label\n",
    "\n",
    "    # Use regex to get number after colon for every column other than label\n",
    "    features = df.iloc[:, 1:].applymap(lambda x: float(re.findall(r':(.*)', x)[0]))\n",
    "\n",
    "    # Put features and labels in same dataframe\n",
    "    df = features\n",
    "    df['label'] = labels\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df, repeat_importance, two_sided, delta_features):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    n_rows = 0\n",
    "    max_diff = 4\n",
    "    n_features = 136\n",
    "    \n",
    "    # Find max number of rows: n_queries * n_urls_per_query ^ 2 * max_repeat_factor\n",
    "    for qid in df.query_id.unique():\n",
    "        urls_per_query = df[df.query_id == qid].shape[0]\n",
    "        n_rows += (urls_per_query ** 2) * max_diff\n",
    "    \n",
    "    # Add extra set of columns if delta_features, + 4 for i, j, query_id, label\n",
    "    if delta_features:\n",
    "        n_columns = n_features * 3 + 4\n",
    "    else:\n",
    "        n_columns = n_features * 2 + 4\n",
    "    \n",
    "    # Create array to fill in later (faster)\n",
    "    features = np.full(shape=(n_rows, n_columns), fill_value=np.nan)\n",
    "    idx = 0\n",
    "    \n",
    "    # Compare each URL for a given query\n",
    "    for progress, qid in enumerate(df.query_id.unique()):\n",
    "        \n",
    "        # tdf: temporary dataframe, m: number of URLs in tdf\n",
    "        tdf = df[df.query_id == qid]\n",
    "        m = tdf.shape[0]\n",
    "        \n",
    "        # First URL\n",
    "        for i in range(m):\n",
    "            \n",
    "            # Two sided: feature (a, b) will be repeated later as feature (b, a)\n",
    "            if two_sided:\n",
    "                start_j = 0\n",
    "            else:\n",
    "                start_j = i\n",
    "            \n",
    "            # Second URL\n",
    "            for j in range(start_j, m):\n",
    "                \n",
    "                label_diff = tdf.label.iloc[i] - tdf.label.iloc[j]\n",
    "                \n",
    "                # Repeat importance: duplicate row |label_diff| times\n",
    "                if repeat_importance:\n",
    "                    end_k = 1\n",
    "                else:\n",
    "                    end_k = int(abs(label_diff)) + 1\n",
    "                    \n",
    "                for k in range(end_k):\n",
    "                    \n",
    "                    # Delta features: for feature (a, b), represent as (a, b, a-b)\n",
    "                    # Format: (i, j, query_id, URLi, URLj, URLi-URLj (?), label_diff)\n",
    "                    if delta_features:\n",
    "                        new_row = np.hstack((i,\n",
    "                                             j,\n",
    "                                             qid,\n",
    "                                             tdf.iloc[i, 1:-1], \n",
    "                                             tdf.iloc[j, 1:-1], \n",
    "                                             tdf.iloc[i, 1:-1] - tdf.iloc[j, 1:-1],  \n",
    "                                             label_diff))\n",
    "                    else:\n",
    "                            new_row = np.hstack((i,\n",
    "                                                 j,\n",
    "                                                 qid,\n",
    "                                                 tdf.iloc[i, 1:-1], \n",
    "                                                 tdf.iloc[j, 1:-1],  \n",
    "                                                 label_diff))\n",
    "                        \n",
    "                    features[idx] = new_row\n",
    "                    idx += 1\n",
    "\n",
    "        print(progress)\n",
    "        \n",
    "    features = features[~np.isnan(features[:, 0])]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(features, ):\n",
    "    # Includes query_id in features\n",
    "    X = features[:, 2:-1]\n",
    "    y = features[:, -1]\n",
    "    \n",
    "    # Same parameters for all calls to ensure consistency\n",
    "    xgbr = XGBRegressor(max_depth=6, \n",
    "                        learning_rate=0.1,\n",
    "                        n_estimators=10, # CHANGE ?\n",
    "                        objective='reg:squarederror')\n",
    "    xgbr.fit(X, y)\n",
    "    \n",
    "    # Want to make predictions on every URL pair within a query, for all queries\n",
    "    # Avoid predicting on rows that were repeated above\n",
    "    # Combo of i, j query_id ensures that unique will work\n",
    "    feat_unique = np.unique(features[:, :-1], axis=0)\n",
    "    X_unique = feat_unique[:, 2:]\n",
    "    y_unique = feat_unique[:, -1]\n",
    "    y_pred = xgbr.predict(X_unique)\n",
    "    \n",
    "    # idx stores progress thru y_pred\n",
    "    idx = 0\n",
    "    \n",
    "    # For each query, make a prediction matrix\n",
    "    for qid in np.unique(X_unique[:, 0]):\n",
    "        \n",
    "        m = np.sum(X_unique[:, 0] == qid)\n",
    "        mat = np.full(shape=(m, m), fill_value=np.nan)\n",
    "        \n",
    "        # Fill in prediction matrix\n",
    "        for i in range(m * m):\n",
    "            \n",
    "            (a, b) = feat_unique[feat_unique[:, 2] == qid][i, :2] \n",
    "            mat[a, b] = y_pred[idx]\n",
    "            idx += 1\n",
    "        \n",
    "        # Apply logistic function\n",
    "        mat = expit(mat)\n",
    "        \n",
    "        # Sum across rows to get 'power' of each individual training example\n",
    "        # Get ranking using the scores as indices\n",
    "        scores = np.sum(mat, axis=0)\n",
    "        ranking = np.argsort(scores)\n",
    "        \n",
    "        # Apply ranking to original labels\n",
    "        r = y_unique[ranking]\n",
    "        print('Query %d, m=%d, NDCG=%.4f' % (qid, m, ndcg_at_k(r=r, k=m)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = generate_dataframe('/Users/Ashtekar15/Desktop/Thesis/MGBoost/other/test_data/ranking/MSLR-WEB10K/Fold1/vali.txt', \n",
    "                           n_queries=3, \n",
    "                           seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "my_f = generate_features(my_df, \n",
    "                         repeat_importance=False, \n",
    "                         two_sided=True, \n",
    "                         delta_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107391, 412)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = my_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Includes query_id in features\n",
    "# X = features[:, 2:-1]\n",
    "# y = features[:, -1]\n",
    "\n",
    "# # # Same parameters for all calls to ensure consistency\n",
    "# # xgbr = XGBRegressor(max_depth=6, \n",
    "# #                     learning_rate=0.1,\n",
    "# #                     n_estimators=30,\n",
    "# #                     objective='reg:squarederror')\n",
    "# # xgbr.fit(X, y)\n",
    "\n",
    "# # Want to make predictions on every URL pair within a query, for all queries\n",
    "# # Avoid predicting on rows that were repeated above\n",
    "# # Combo of i, j query_id ensures that unique will work\n",
    "# X_unique = np.unique(features[:, :-1], axis=0)\n",
    "# X_unique = X_unique[:, 2:]\n",
    "# # y_pred = xgbr.predict(X_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3535., 15925., 28990.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.unique(X_unique[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28561.5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (np.sum(my_df.query_id == 3535) ** 2 + np.sum(my_df.query_id == 15925) ** 2 + np.sum(my_df.query_id == 28990) ** 2) / 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8646\n",
      "8646\n",
      "11476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28768"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count = 0\n",
    "# for qid in np.unique(X_unique[:, 0]):\n",
    "#     print(np.sum(X_unique[:, 0] == qid))\n",
    "#     count += np.sum(X_unique[:, 0] == qid)\n",
    "# count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(my_df.query_id == qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 3535, m=8646, NDCG=0.0427\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-71cb9beab293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat_unique\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat_unique\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mqid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Includes query_id in features\n",
    "X = features[:, 2:-1]\n",
    "y = features[:, -1]\n",
    "\n",
    "# Same parameters for all calls to ensure consistency\n",
    "xgbr = XGBRegressor(max_depth=6, \n",
    "                    learning_rate=0.1,\n",
    "                    n_estimators=100, # CHANGE ?\n",
    "                    objective='reg:squarederror')\n",
    "xgbr.fit(X, y)\n",
    "\n",
    "print('Model fitted')\n",
    "\n",
    "# Want to make predictions on every URL pair within a query, for all queries\n",
    "# Avoid predicting on rows that were repeated above\n",
    "# Combo of i, j query_id ensures that unique will work\n",
    "feat_unique = np.unique(features, axis=0)\n",
    "X_unique = feat_unique[:, 2:-1]\n",
    "y_unique = feat_unique[:, -1]\n",
    "y_pred = xgbr.predict(X_unique)\n",
    "\n",
    "# idx stores progress thru y_pred\n",
    "idx = 0\n",
    "\n",
    "# For each query, make a prediction matrix\n",
    "for qid in np.unique(X_unique[:, 0]):\n",
    "\n",
    "    m = np.sum(X_unique[:, 0] == qid)\n",
    "    mat = np.full(shape=(m, m), fill_value=np.nan)\n",
    "\n",
    "    # Fill in prediction matrix\n",
    "    for i in range(m):\n",
    "\n",
    "        (a, b) = feat_unique[feat_unique[:, 2] == qid][i, :2] \n",
    "        mat[int(a), int(b)] = y_pred[idx]\n",
    "        idx += 1\n",
    "\n",
    "    # Apply logistic function\n",
    "    mat = expit(mat)\n",
    "\n",
    "    # Sum across rows to get 'power' of each individual training example\n",
    "    # Get ranking using the scores as indices\n",
    "    scores = np.sum(mat, axis=0)\n",
    "    ranking = np.argsort(scores)\n",
    "\n",
    "    # Apply ranking to original labels\n",
    "    r = y_unique[X_unique[:, 0] == qid][ranking]\n",
    "    print('Query %d, m=%d, NDCG=%.4f' % (qid, m, ndcg_at_k(r=r, k=m)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
