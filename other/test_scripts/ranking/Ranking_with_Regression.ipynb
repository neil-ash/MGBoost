{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking using regression\n",
    "\n",
    "Implementation of pairwise ranking through regression, using xgboost.\n",
    "\n",
    "****\n",
    "\n",
    "**Plan:**\n",
    "- Generate data\n",
    "    - Features: like previously (random normal)\n",
    "    - Labels: uniform at random from (20, 40, 80, 100, 120)\n",
    "        - **This takes a long time: would need to replicate training data 50 (?) times on average**\n",
    "- Setup training data\n",
    "    - Xi, Xj, Xi - Xj, yi - yj\n",
    "    - For all possible pairs of feaure rows\n",
    "    - **(?) Repeat training data |yi-yj| times (?)**\n",
    "        - Do this for classification...\n",
    "    - Arrange labels in m x m matrix (?)\n",
    "        - For later step\n",
    "        - **No -- only applicable for making predictions**\n",
    "- Learn xgboost regressor on training data\n",
    "- Output predictions from xgboost regressor\n",
    "    - Store predictions in m x m matrix\n",
    "    - Apply sigmoid (logit) function to scores in predictions matrix\n",
    "    - Sum across rows (?) in predictions matrix to get final score for each row\n",
    "    - Use each final score to rank\n",
    "    \n",
    "****\n",
    "\n",
    "**Questions:**\n",
    "- Repeat training data |yi-yj| times for regression, or just for classification?\n",
    "- Sampled labels uniformly at random from (1, 2, 3, 4, 5) rather than (20, 40, 80, 100, 120) since the later would require many more duplications. Is this fine?\n",
    "- Used m = 1000 (1000 training examples). Is this fine?\n",
    "- I did not train/test split the data. Is this fine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.special import expit  # Logistic function\n",
    "from rank_metrics import ndcg_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducible results\n",
    "np.random.seed(1)\n",
    "\n",
    "# Dimensions of generated data\n",
    "m = 1000    # Training examples\n",
    "n = 5       # Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features randomly (gaussian)\n",
    "X = np.random.normal(loc=0, scale=1, size=(m, n))\n",
    "\n",
    "# Generate labels from uniform distribution with given values\n",
    "# y_values = [20, 40, 80, 100, 120]\n",
    "y_values = [1, 2, 3, 4, 5]\n",
    "y = np.random.choice(a=y_values, size=m, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "# Will repeat each row (pairwise feature comparison) |yi - yj| times\n",
    "# First construct NaN array, then fill in (potentially faster)\n",
    "# When constructing NaN array, fill in with 'worst-case' (every row repeated max times)\n",
    "max_diff = max(y_values) - min(y_values)\n",
    "train = np.full(shape=(m * m * max_diff, 3 * n + 3), fill_value=np.nan)\n",
    "idx = 0\n",
    "\n",
    "# First feature\n",
    "for i in range(m):\n",
    "    \n",
    "    # Second feature\n",
    "    for j in range(m):\n",
    "        \n",
    "        y_diff = y[i] - y[j]\n",
    "\n",
    "        # Repeat each row |yi - yj| times, need to include at least once\n",
    "        for k in range(int(abs(y_diff)) + 1):\n",
    "            \n",
    "            # Store first index, second index, first feature, second feature, \n",
    "            # feature difference, and label difference\n",
    "            row_new = np.hstack(([i, j], X[i, :], X[j, :], X[i, :] - X[j, :], y_diff))\n",
    "            train[idx] = row_new\n",
    "            idx += 1\n",
    "    \n",
    "    # See progress\n",
    "    if i % (m / 10) == 0: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2647956, (2647956, 18))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure shapes match up\n",
    "idx, train[~np.isnan(train[:, 0])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only store part of array filled in thru iteration\n",
    "train = train[~np.isnan(train[:, 0])]\n",
    "\n",
    "# Store new features and labels to actually train model\n",
    "X_new = train[:, 2:-1]\n",
    "y_new = train[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ashtekar15/anaconda3/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=6, min_child_weight=1, missing=None, n_estimators=10,\n",
       "       n_jobs=1, nthread=None, objective='reg:squarederror',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default parameters except max_depth (default 3), n_estimators (default 100), \n",
    "# objective (default: 'reg:linear', deprecated)\n",
    "xgbr = XGBRegressor(max_depth=6, \n",
    "                    learning_rate=0.1,\n",
    "                    n_estimators=10,\n",
    "                    objective='reg:squarederror')\n",
    "xgbr.fit(X_new, y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make predictions, get unique training features along with indices i, j \n",
    "# as first two columns\n",
    "X_idx = train[:, :-1]\n",
    "X_idx = np.unique(X_idx, axis=0)\n",
    "\n",
    "# Make predictions on data not including indices\n",
    "y_pred = xgbr.predict(X_idx[:, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairwise ranking matrix to fill in later\n",
    "mat = np.full(shape=(m, m), fill_value=np.nan)\n",
    "\n",
    "# Fill in ranking matrix\n",
    "for i in range(m * m):\n",
    "    mat[int(X_idx[i, 0]), int(X_idx[i, 1])] = y_pred[i]\n",
    "\n",
    "# Apply logistic function \n",
    "mat = expit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53483967, 0.51457834, 0.53391171, 0.55346845, 0.55940523,\n",
       "       0.5401626 , 0.55142085, 0.55136268, 0.55910923, 0.55502063,\n",
       "       0.54611872, 0.52765327, 0.5401626 , 0.57655319, 0.55091207,\n",
       "       0.54124595, 0.53649706, 0.56040828, 0.53483967, 0.53483967,\n",
       "       0.53698847, 0.54217587, 0.55346845, 0.55091207, 0.58553529,\n",
       "       0.53483967, 0.55091207, 0.55820419, 0.55091207, 0.55215229,\n",
       "       0.42844657, 0.55142085, 0.5401626 , 0.45144239, 0.46161585,\n",
       "       0.53698847, 0.53973858, 0.52145534, 0.59924975, 0.56477312,\n",
       "       0.48707164, 0.49701848, 0.55142085, 0.55012457, 0.51691972,\n",
       "       0.54889208, 0.54054971, 0.56505871, 0.51428816, 0.54054971,\n",
       "       0.50486452, 0.53698847, 0.53483967, 0.52556295, 0.51625583,\n",
       "       0.58151676, 0.55820419, 0.54217587, 0.54748589, 0.5401626 ,\n",
       "       0.54889208, 0.54791149, 0.57886133, 0.54611872, 0.5401626 ,\n",
       "       0.53741646, 0.56304411, 0.48789411, 0.51461653, 0.53698847,\n",
       "       0.54791149, 0.55142085, 0.49913488, 0.55147246, 0.53741646,\n",
       "       0.48139232, 0.53483967, 0.55142085, 0.53698847, 0.57886133,\n",
       "       0.53973858, 0.55401604, 0.55142085, 0.51457834, 0.49743331,\n",
       "       0.54611872, 0.57712939, 0.53907259, 0.49938576, 0.53741646,\n",
       "       0.48542436, 0.56466347, 0.55820419, 0.53698847, 0.51848311,\n",
       "       0.50006418, 0.54649776, 0.51625583, 0.53698847, 0.51089435])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values along diagonal should be about 0.5 (error due to model)\n",
    "diag = np.diagonal(mat)\n",
    "diag[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5421178425025636,\n",
       " 0.08675644746204147,\n",
       " 0.13441176174041777,\n",
       " 0.894677791903511)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stats for entire matrix (to compare with diagonal)\n",
    "np.mean(mat), np.std(mat), np.min(mat), np.max(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5422277349882161,\n",
       " 0.025801931383629677,\n",
       " 0.4139760064306653,\n",
       " 0.6607326172740777)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stats for diagonal of matrix, should be more closely grouped around 0.5 vs entire matrix\n",
    "np.mean(diag), np.std(diag), np.min(diag), np.max(diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([853, 389,  40, 526, 879, 609, 136,  50, 587, 925])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum across rows to get 'power' of each individual training example\n",
    "# (total rank score relative to the rest of the training examples)\n",
    "scores = np.sum(mat, axis=0)\n",
    "ranking = np.argsort(scores)\n",
    "ranking[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542.1178425025632, 63.36784558275654, 224.6707733421218, 819.3795738262625)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stats for scores\n",
    "np.mean(scores), np.std(scores), np.min(scores), np.max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224.6707733421218, 819.3795738262625)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm max and min ranking is consistent with above\n",
    "scores[ranking[0]], scores[ranking[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 4, 4,\n",
       "       5, 5, 2, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 4, 4, 4, 3, 4,\n",
       "       4, 4, 5, 4, 4, 4, 5, 3, 2, 5, 4, 5, 2, 5, 5, 5, 4, 5, 5, 4, 3, 4,\n",
       "       5, 2, 4, 5, 3, 5, 3, 5, 5, 5, 2, 5, 5, 5, 4, 5, 5, 5, 3, 5, 2, 4,\n",
       "       5, 5, 4, 4, 1, 4, 5, 5, 4, 3, 5, 5, 3, 4, 5, 5, 4, 5, 3, 5, 3, 2,\n",
       "       5, 3, 5, 5, 4, 1, 4, 5, 4, 3, 5, 4, 3, 1, 3, 5, 4, 3, 3, 5, 5, 5,\n",
       "       5, 5, 5, 4, 5, 5, 3, 4, 2, 5, 5, 4, 4, 5, 5, 5, 5, 5, 3, 4, 4, 3,\n",
       "       3, 5, 4, 3, 4, 4, 4, 5, 3, 4, 5, 5, 5, 2, 1, 3, 4, 2, 3, 5, 5, 4,\n",
       "       4, 3, 3, 4, 3, 5, 5, 5, 4, 4, 4, 4, 5, 3, 4, 5, 2, 5, 2, 2, 2, 5,\n",
       "       5, 1, 1, 4, 2, 3, 3, 5, 2, 5, 4, 4, 4, 4, 1, 4, 4, 5, 4, 5, 2, 5,\n",
       "       1, 4, 2, 3, 3, 5, 1, 4, 2, 4, 5, 4, 3, 5, 4, 4, 4, 5, 5, 5, 2, 4,\n",
       "       5, 2, 5, 3, 2, 4, 4, 4, 3, 4, 5, 3, 5, 3, 2, 3, 4, 4, 5, 5, 5, 5,\n",
       "       4, 4, 1, 5, 4, 5, 3, 4, 2, 3, 3, 2, 3, 4, 3, 5, 5, 4, 4, 5, 4, 5,\n",
       "       5, 4, 5, 4, 2, 5, 3, 2, 5, 3, 5, 4, 3, 5, 4, 4, 5, 3, 4, 1, 2, 4,\n",
       "       2, 5, 3, 2, 4, 5, 4, 3, 5, 3, 1, 2, 3, 1, 2, 3, 4, 2, 1, 2, 5, 1,\n",
       "       4, 2, 2, 5, 3, 2, 2, 3, 5, 4, 3, 4, 3, 5, 2, 2, 3, 2, 1, 4, 3, 1,\n",
       "       4, 5, 5, 5, 4, 3, 4, 5, 3, 4, 5, 4, 1, 4, 3, 4, 2, 4, 3, 5, 3, 4,\n",
       "       2, 4, 2, 4, 3, 4, 4, 4, 4, 2, 3, 4, 4, 4, 5, 1, 5, 1, 5, 1, 1, 1,\n",
       "       4, 2, 2, 2, 3, 3, 3, 5, 2, 3, 5, 2, 4, 3, 1, 3, 4, 5, 4, 4, 2, 4,\n",
       "       1, 3, 5, 2, 1, 3, 3, 4, 4, 1, 3, 3, 2, 1, 4, 5, 1, 3, 4, 4, 2, 2,\n",
       "       3, 2, 3, 3, 5, 4, 1, 2, 3, 4, 1, 1, 4, 2, 3, 5, 3, 1, 5, 5, 4, 5,\n",
       "       1, 4, 1, 1, 1, 3, 4, 2, 4, 1, 3, 1, 2, 2, 2, 2, 1, 3, 4, 1, 5, 2,\n",
       "       5, 1, 1, 4, 1, 4, 5, 5, 5, 1, 1, 2, 4, 4, 1, 4, 5, 2, 1, 4, 3, 2,\n",
       "       2, 2, 5, 1, 4, 3, 2, 3, 4, 4, 1, 5, 3, 2, 1, 4, 3, 5, 3, 4, 4, 3,\n",
       "       4, 3, 1, 4, 5, 4, 2, 1, 4, 5, 1, 2, 2, 1, 5, 5, 3, 1, 1, 3, 1, 2,\n",
       "       3, 1, 3, 3, 2, 4, 3, 1, 5, 2, 1, 5, 3, 2, 3, 5, 5, 4, 4, 5, 5, 2,\n",
       "       2, 3, 2, 1, 4, 4, 5, 1, 4, 2, 3, 2, 3, 3, 2, 1, 2, 4, 2, 1, 4, 5,\n",
       "       2, 3, 2, 1, 1, 1, 4, 2, 1, 5, 4, 5, 5, 4, 2, 2, 4, 3, 5, 4, 1, 4,\n",
       "       3, 4, 4, 3, 3, 1, 3, 5, 4, 3, 5, 3, 3, 4, 3, 4, 2, 4, 5, 4, 3, 1,\n",
       "       1, 2, 3, 3, 1, 5, 1, 2, 4, 4, 5, 1, 1, 4, 1, 3, 2, 3, 2, 5, 1, 5,\n",
       "       1, 3, 1, 4, 3, 1, 2, 5, 5, 1, 2, 1, 4, 3, 1, 4, 3, 1, 2, 5, 3, 5,\n",
       "       2, 2, 2, 3, 5, 4, 3, 2, 1, 3, 4, 4, 3, 3, 3, 4, 5, 1, 3, 2, 3, 4,\n",
       "       1, 1, 5, 3, 4, 1, 4, 2, 1, 5, 1, 4, 3, 1, 5, 1, 2, 2, 1, 5, 3, 5,\n",
       "       1, 1, 1, 1, 2, 3, 1, 3, 5, 2, 2, 3, 1, 1, 3, 2, 4, 4, 1, 2, 1, 3,\n",
       "       2, 4, 3, 2, 3, 5, 4, 1, 1, 3, 4, 2, 4, 1, 1, 3, 4, 1, 1, 2, 3, 2,\n",
       "       3, 4, 1, 5, 3, 3, 5, 2, 2, 3, 2, 1, 1, 1, 4, 1, 3, 3, 3, 1, 5, 4,\n",
       "       3, 4, 2, 1, 1, 3, 1, 5, 1, 1, 4, 3, 5, 2, 3, 2, 2, 1, 5, 1, 2, 1,\n",
       "       1, 1, 3, 2, 1, 5, 1, 2, 4, 1, 1, 1, 4, 5, 3, 4, 3, 1, 1, 1, 1, 1,\n",
       "       1, 3, 3, 3, 3, 1, 3, 2, 2, 1, 4, 3, 3, 3, 5, 1, 1, 3, 2, 4, 3, 2,\n",
       "       2, 2, 3, 2, 2, 3, 2, 2, 1, 4, 1, 1, 3, 1, 3, 4, 4, 3, 2, 1, 2, 3,\n",
       "       2, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 3,\n",
       "       2, 2, 2, 3, 1, 1, 2, 3, 1, 1, 1, 4, 2, 1, 1, 3, 2, 4, 3, 2, 1, 1,\n",
       "       3, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 4, 1, 1, 2, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 1, 1, 1, 2,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2,\n",
       "       2, 1, 2, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if ranking results make sense w.r.t. original labels\n",
    "y[ranking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9739155100075407"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NDCG metric, 0 is worst and 1 is best (?)\n",
    "r = y[ranking]\n",
    "ndcg_at_k(r=r, k=m)  # 'method' parameter in function - has to do with weighting (?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
