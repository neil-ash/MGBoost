{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahoo Data Processing\n",
    "\n",
    "Testing the performance of various feature configurations when using DeltaMART with the Yahoo LETOR dataset (train/validation/test split):\n",
    "\n",
    "https://github.com/QingyaoAi/Unbiased-Learning-to-Rank-with-Unbiased-Propensity-Estimation\n",
    "\n",
    "Only using a small subset of queries (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils  # To load Yahoo dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.special import expit  # Logistic function\n",
    "from rank_metrics import ndcg_at_k, mean_average_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data\n",
    "\n",
    "#### Features: train.feature\n",
    "\n",
    "Description: \"test_2_5\" means the 5th document for the query with identifier \"2\" in the original test set of the Yahoo letor data.\n",
    "\n",
    "Interpretation: first value is test_queryNum_docNum, rest are feature values (svm_light format?)\n",
    "\n",
    "****\n",
    "\n",
    "#### Labels: train.weights\n",
    "\n",
    "Description: The annotated relevance value for documents in the initial list of each query.\n",
    "\n",
    "Interpretation: first value is queryNum (query_id), rest are labels for URLs at corresponding indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data_utils.read_data(data_path='/Users/Ashtekar15/Desktop/Thesis/MGBoost/other/test_data/generate_dataset/',\n",
    "#                             file_prefix='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats for validation set\n",
    "\n",
    "**dids** (71083): valid_19945_15..., stores query/URL id info\n",
    "\n",
    "**qids** (2994): stores query id info\n",
    "\n",
    "**features** (71083): list of lists, each sublist is a given query/URL pair (sublist len 700)\n",
    "\n",
    "**gold_weights** (2994): list of lists, each sublist is the labels for URLs of a single query (sublist len varies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Num queries in train/val/test (should be 29921)\n",
    "# 19944 + 2994 + 6983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Get info on number of URLs/query\n",
    "# lls = []\n",
    "\n",
    "# for ls in data.gold_weights:\n",
    "#     lls.append(len(ls))\n",
    "\n",
    "# np.mean(lls), np.std(lls), min(lls), max(lls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Solve for num_queries\n",
    "# num_queries = 10\n",
    "\n",
    "# mean = np.mean(lls)\n",
    "\n",
    "# # (Estimated) total size in GB with given num_queries\n",
    "# (700 * 3) * (mean ** 2) * num_queries * 64 / (10 ** 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To get total number of values FOR ENTIRE TRAIN SET thru feature generation\n",
    "# total = 0\n",
    "# for ls in data.gold_weights:\n",
    "#     total += (700 * 3) * (len(ls) ** 2)\n",
    "\n",
    "# # Estimation of total size in GB\n",
    "# (total * 64) / (10 ** 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan:**\n",
    "- Convert np arrays to np.float32 (?)\n",
    "    - To save memory/use more training data\n",
    "- Choose 10 queries randomly\n",
    "    - Use seed\n",
    "- Get features and labels corresponding to these queries\n",
    "    - Include query id in features (?)\n",
    "    - data.features, data.goldlist\n",
    "- Generate pairwise features\n",
    "    - Include option for delta_features\n",
    "- Build model/make predictions on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert lists to np arrays for faster access\n",
    "\n",
    "# # String\n",
    "# dids = np.array(data.dids)\n",
    "\n",
    "# # String -> int\n",
    "# qids = np.array(data.qids, dtype=int)\n",
    "\n",
    "# # float64 -> float32\n",
    "# features = np.array(data.features, dtype=np.float32)\n",
    "\n",
    "# # Since not all sublists of same size\n",
    "# gold_weights = np.array([np.array(x, dtype=np.float32) for x in data.gold_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' SAVING ENTIRE DATASET '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" SAVING ENTIRE DATASET \"\"\"\n",
    "# folder = 'Yahoo_Train//'\n",
    "\n",
    "# np.save(folder + 'dids.npy', dids)\n",
    "# np.save(folder + 'qids.npy', qids)\n",
    "# np.save(folder + 'features.npy', features)\n",
    "# np.save(folder + 'gold_weights.npy', gold_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run imports and cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOADING ENTIRE DATASET \"\"\"\n",
    "np.random.seed(5)\n",
    "delta_features = True\n",
    "dataset = 'val'\n",
    "\n",
    "if dataset == 'train':\n",
    "    folder = 'Yahoo_Train//' \n",
    "    size = 10\n",
    "    repeat_importance = True\n",
    "elif dataset == 'val':\n",
    "    folder = 'Yahoo_Val//'\n",
    "    size = 5\n",
    "    repeat_importance = False\n",
    "else:\n",
    "    print('Choose train or val dataset')\n",
    "\n",
    "dids = np.load(folder + 'dids.npy')\n",
    "qids = np.load(folder + 'qids.npy')\n",
    "features = np.load(folder + 'features.npy')\n",
    "gold_weights = np.load(folder + 'gold_weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 10 queries (train) or 5 queries (val)\n",
    "q_choice = np.random.choice(qids, size=size)\n",
    "\n",
    "# Get query id aligned with features\n",
    "query_id = np.array([int(ele.split(\"_\")[1]) for ele in dids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant queries, features, and labels\n",
    "q_rel = query_id[np.isin(query_id, q_choice)]\n",
    "feat_rel = features[np.isin(query_id, q_choice)]\n",
    "label_rel = gold_weights[np.isin(qids, q_choice)]\n",
    "\n",
    "# Join subarrays\n",
    "label_rel = np.concatenate(label_rel)\n",
    "\n",
    "# Include query id in features\n",
    "feat_rel = np.hstack((q_rel.reshape(-1, 1), feat_rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important to free up memory\n",
    "# del data\n",
    "del dids, qids, features, gold_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "n_rows = 0\n",
    "max_diff = 4\n",
    "n_features = 700\n",
    "\n",
    "# Find max possible number of rows: n_queries * (n_urls_per_query ^ 2) * max_repeat_factor\n",
    "for qid in q_choice:\n",
    "    urls_per_query = np.sum(np.isin(q_rel, qid))\n",
    "    \n",
    "    # If not repeating importance, then every query-URL pair only appears once\n",
    "    if repeat_importance:\n",
    "        n_rows += (urls_per_query ** 2) * max_diff\n",
    "    else:\n",
    "        n_rows += (urls_per_query ** 2)\n",
    "    \n",
    "# Add extra set of columns if delta_features, + 2 for (query_id, label)\n",
    "if delta_features:\n",
    "    n_columns = (n_features * 3) + 2\n",
    "else:\n",
    "    n_columns = (n_features * 2) + 2\n",
    "\n",
    "# Create array to fill in later (faster), step thru with idx\n",
    "features = np.full(shape=(n_rows, n_columns), fill_value=np.nan)\n",
    "idx = 0\n",
    "\n",
    "# Iter thru queries\n",
    "for progress, qid in enumerate(q_choice):\n",
    "    \n",
    "    temp_feat = feat_rel[np.isin(q_rel, qid)]\n",
    "    temp_label = label_rel[np.isin(q_rel, qid)]\n",
    "    \n",
    "    m = temp_feat.shape[0]\n",
    "    \n",
    "    # First URL\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Second URL\n",
    "        for j in range(m):\n",
    "            \n",
    "            label_diff = temp_label[i] - temp_label[j]\n",
    "            \n",
    "            # Repeat importance: duplicate row |label_diff| times\n",
    "            if repeat_importance:\n",
    "                end_k = int(abs(label_diff)) + 1\n",
    "            else:\n",
    "                end_k = 1\n",
    "\n",
    "            for k in range(end_k):\n",
    "\n",
    "                # Delta features: for feature (a, b), represent as (a, b, a-b)\n",
    "                # Format: (qid, feat[i], feat[j], feat[i] - feat[j], label_diff)\n",
    "                if delta_features:\n",
    "                    new_row = np.hstack((temp_feat[i], \n",
    "                                         temp_feat[j, 1:], \n",
    "                                         temp_feat[i, 1:] - temp_feat[j, 1:],\n",
    "                                         label_diff))\n",
    "                else:\n",
    "                    new_row = np.hstack((temp_feat[i], \n",
    "                                         temp_feat[j, 1:], \n",
    "                                         label_diff))\n",
    "\n",
    "                features[idx] = new_row\n",
    "                idx += 1\n",
    "\n",
    "    print(progress + 1)\n",
    "    \n",
    "# Originally allocated array is likely too large, only save relevant rows\n",
    "features = features[~np.isnan(features[:, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25817, 2102) (1271, 2102)\n",
      "Ready to build model\n"
     ]
    }
   ],
   "source": [
    "# Save train/val features\n",
    "if dataset == 'train':\n",
    "    train_feat = features\n",
    "    print('Now repeat with val dataset')\n",
    "elif dataset == 'val':\n",
    "    test_feat = features  # Rename to work with model building\n",
    "    print(train_feat.shape, test_feat.shape)\n",
    "    print('Ready to build model')\n",
    "else:\n",
    "    print('Choose train or val dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitted\n",
      "Query 21369, m=9:\n",
      "\tMAP:     1.0000\n",
      "\tNDCG@1:  1.0000\n",
      "\tNDCG@3:  0.7841\n",
      "\tNDCG@5:  0.9101\n",
      "\tNDCG@10: 0.9264\n",
      "\tNDCG@m:  0.9264\n",
      "Query 21670, m=11:\n",
      "\tMAP:     0.9809\n",
      "\tNDCG@1:  1.0000\n",
      "\tNDCG@3:  0.7934\n",
      "\tNDCG@5:  0.7886\n",
      "\tNDCG@10: 0.8208\n",
      "\tNDCG@m:  0.8919\n",
      "Query 22066, m=12:\n",
      "\tMAP:     0.6111\n",
      "\tNDCG@1:  1.0000\n",
      "\tNDCG@3:  0.5000\n",
      "\tNDCG@5:  0.5000\n",
      "\tNDCG@10: 0.6577\n",
      "\tNDCG@m:  0.6577\n",
      "Query 22199, m=5:\n",
      "\tMAP:     0.0000\n",
      "\tNDCG@1:  0.0000\n",
      "\tNDCG@3:  0.0000\n",
      "\tNDCG@5:  0.0000\n",
      "\tNDCG@10: 0.0000\n",
      "\tNDCG@m:  0.0000\n",
      "Query 22860, m=30:\n",
      "\tMAP:     0.9494\n",
      "\tNDCG@1:  1.0000\n",
      "\tNDCG@3:  0.5692\n",
      "\tNDCG@5:  0.5949\n",
      "\tNDCG@10: 0.6405\n",
      "\tNDCG@m:  0.8044\n",
      "\n",
      "Overall:\n",
      "\tMAP:     0.7083\n",
      "\tNDCG@1:  0.8000\n",
      "\tNDCG@3:  0.5293\n",
      "\tNDCG@5:  0.5587\n",
      "\tNDCG@10: 0.6091\n",
      "\tNDCG@m:  0.6561\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TRAIN \"\"\"\n",
    "# Features does not include i, j, does includes query_id\n",
    "X_train = train_feat[:, :-1]\n",
    "y_train = train_feat[:, -1]\n",
    "\n",
    "# Same parameters for all calls to ensure consistency\n",
    "xgbr = XGBRegressor(max_depth=6, \n",
    "                    learning_rate=0.1,\n",
    "                    n_estimators=100, # Change to make faster OR more powerful (?)\n",
    "                    objective='reg:squarederror')\n",
    "\n",
    "xgbr.fit(X_train, y_train)\n",
    "\n",
    "print('Model fitted')\n",
    "\n",
    "\"\"\" TEST \"\"\"\n",
    "# Want to make predictions on every URL pair within a query, for all queries\n",
    "X_test = test_feat[:, :-1]\n",
    "y_test = test_feat[:, -1]\n",
    "y_pred = xgbr.predict(X_test)\n",
    "\n",
    "# Record results over all queries\n",
    "MAP = 0\n",
    "NDCG1, NDCG3, NDCG5, NDCG10, NDCGM = 0, 0, 0, 0, 0\n",
    "\n",
    "# Save rankings (to visually compare)\n",
    "r_ls = []\n",
    "\n",
    "# For each query, make a prediction array (scores)\n",
    "for qid in np.unique(X_test[:, 0]):\n",
    "\n",
    "    # m will be the number of URLs per given query ID\n",
    "    m = int(np.sqrt(np.sum(X_test[:, 0] == qid)))\n",
    "\n",
    "    # Save y_pred only for query of interest as y_pq, reshape in order to sum across rows\n",
    "    # Note that the default order='C' in reshape is fine (row-major)\n",
    "    # Setting order='F' will result in roughly the same result, just reversed since the \n",
    "    # learned labels correspond to (URLi - URLj)\n",
    "    y_pq = y_pred[X_test[:, 0] == qid]\n",
    "    y_pq = y_pq.reshape(m, m, order='C')\n",
    "\n",
    "    # Apply logistic function\n",
    "    y_pq = expit(y_pq)\n",
    "\n",
    "    # Sum across rows to get 'power' of each individual training example\n",
    "    # Get order using the scores as indices\n",
    "    scores = np.sum(y_pq, axis=0)\n",
    "    order = np.argsort(scores)\n",
    "\n",
    "    # Apply order to original labels\n",
    "    y_orig = label_rel[feat_rel[:, 0] == qid]\n",
    "    r = y_orig[order]\n",
    "    \n",
    "    # Save ranking\n",
    "    r_ls.append(r)\n",
    "\n",
    "    # Get results\n",
    "    m_a_p = mean_average_precision([r])\n",
    "    n1, n3, n5, n10, nm = ndcg_at_k(r=r, k=1),ndcg_at_k(r=r, k=3), ndcg_at_k(r=r, k=5), ndcg_at_k(r=r, k=10), ndcg_at_k(r=r, k=m)\n",
    "\n",
    "    # Update overall results\n",
    "    MAP += m_a_p\n",
    "    NDCG1 += n1\n",
    "    NDCG3 += n3\n",
    "    NDCG5 += n5\n",
    "    NDCG10 += n10\n",
    "    NDCGM += nm\n",
    "\n",
    "    # Results for query\n",
    "    print('Query %d, m=%d:' % (qid, m))\n",
    "    print('\\tMAP:     %.4f' % m_a_p)\n",
    "    print('\\tNDCG@1:  %.4f' % n1)\n",
    "    print('\\tNDCG@3:  %.4f' % n3)\n",
    "    print('\\tNDCG@5:  %.4f' % n5)\n",
    "    print('\\tNDCG@10: %.4f' % n10)\n",
    "    print('\\tNDCG@m:  %.4f' % nm)\n",
    "\n",
    "# Results over all queries\n",
    "print('\\nOverall:')\n",
    "print('\\tMAP:     %.4f' % (MAP / size))\n",
    "print('\\tNDCG@1:  %.4f' % (NDCG1 / size))\n",
    "print('\\tNDCG@3:  %.4f' % (NDCG3 / size))\n",
    "print('\\tNDCG@5:  %.4f' % (NDCG5 / size))\n",
    "print('\\tNDCG@10: %.4f' % (NDCG10 / size))\n",
    "print('\\tNDCG@m:  %.4f' % (NDCGM / size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4., 2., 2., 4., 2., 2., 2., 2., 1.], dtype=float32),\n",
       " array([3., 2., 2., 2., 1., 1., 1., 2., 0., 3., 3.], dtype=float32),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([4., 1., 1., 1., 2., 1., 1., 1., 2., 2., 2., 0., 3., 2., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 0., 4., 1., 1., 0., 1.], dtype=float32)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ----------------------------------------\n",
    "    Seed = 1, 10/5 train/test query split\n",
    "    \n",
    "    delta_features = False\n",
    "    Overall:\n",
    "        MAP:     0.8772\n",
    "        NDCG@1:  0.7167\n",
    "        NDCG@3:  0.6580\n",
    "        NDCG@5:  0.6551\n",
    "        NDCG@10: 0.7511\n",
    "        NDCG@m:  0.8404\n",
    "        \n",
    "    delta_features = True\n",
    "    Overall:\n",
    "        MAP:     0.9184\n",
    "        NDCG@1:  0.8000\n",
    "        NDCG@3:  0.8254\n",
    "        NDCG@5:  0.7619\n",
    "        NDCG@10: 0.8207\n",
    "        NDCG@m:  0.8969\n",
    "        \n",
    "    ----------------------------------------\n",
    "    Seed = 2, 10/5 train/test query split\n",
    "    \n",
    "    delta_features = False\n",
    "    Overall:\n",
    "        MAP:     0.8719\n",
    "        NDCG@1:  0.6833\n",
    "        NDCG@3:  0.7657\n",
    "        NDCG@5:  0.7592\n",
    "        NDCG@10: 0.7537\n",
    "        NDCG@m:  0.8828\n",
    "        \n",
    "    delta_features = True\n",
    "    Overall:\n",
    "        MAP:     0.8321\n",
    "        NDCG@1:  0.4833\n",
    "        NDCG@3:  0.5534\n",
    "        NDCG@5:  0.5941\n",
    "        NDCG@10: 0.6643\n",
    "        NDCG@m:  0.8118\n",
    "\n",
    "    ----------------------------------------\n",
    "    Seed = 3, 10/5 train/test query split\n",
    "    \n",
    "    delta_features = False\n",
    "    Overall:\n",
    "        MAP:     0.9861\n",
    "        NDCG@1:  0.6000\n",
    "        NDCG@3:  0.7820\n",
    "        NDCG@5:  0.7744\n",
    "        NDCG@10: 0.8527\n",
    "        NDCG@m:  0.8981\n",
    "    \n",
    "    delta_features = True\n",
    "    Overall:\n",
    "        MAP:     0.9971\n",
    "        NDCG@1:  0.7667\n",
    "        NDCG@3:  0.7127\n",
    "        NDCG@5:  0.7652\n",
    "        NDCG@10: 0.8226\n",
    "        NDCG@m:  0.8843\n",
    "        \n",
    "    ----------------------------------------\n",
    "    Seed = 4, 10/5 train/test query split\n",
    "    \n",
    "    delta_features = False\n",
    "    Overall:\n",
    "        MAP:     0.8894\n",
    "        NDCG@1:  0.6833\n",
    "        NDCG@3:  0.6791\n",
    "        NDCG@5:  0.6977\n",
    "        NDCG@10: 0.7723\n",
    "        NDCG@m:  0.8520\n",
    "        \n",
    "    delta_features = True\n",
    "    Overall:\n",
    "        MAP:     0.8644\n",
    "        NDCG@1:  0.6000\n",
    "        NDCG@3:  0.6824\n",
    "        NDCG@5:  0.7199\n",
    "        NDCG@10: 0.7619\n",
    "        NDCG@m:  0.8469\n",
    "        \n",
    "    ----------------------------------------\n",
    "    Seed = 5, 10/5 train/test query split\n",
    "    \n",
    "    delta_features = False\n",
    "    Overall:\n",
    "        MAP:     0.7018\n",
    "        NDCG@1:  0.8000\n",
    "        NDCG@3:  0.5293\n",
    "        NDCG@5:  0.5500\n",
    "        NDCG@10: 0.6009\n",
    "        NDCG@m:  0.6525\n",
    "\n",
    "    delta_features = True\n",
    "    Overall:\n",
    "        MAP:     0.7083\n",
    "        NDCG@1:  0.8000\n",
    "        NDCG@3:  0.5293\n",
    "        NDCG@5:  0.5587\n",
    "        NDCG@10: 0.6091\n",
    "        NDCG@m:  0.6561"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
