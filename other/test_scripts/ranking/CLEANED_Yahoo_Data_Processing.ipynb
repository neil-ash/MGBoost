{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahoo Data Processing\n",
    "\n",
    "Testing the performance of various feature configurations when using DeltaMART with the Yahoo LETOR dataset (train/validation/test split). See README.md, \"Data Preparation for the Initial Ranker\" section:\n",
    "\n",
    "https://github.com/QingyaoAi/Unbiased-Learning-to-Rank-with-Unbiased-Propensity-Estimation\n",
    "\n",
    "Only using a small subset of queries since running locally on laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ashtekar15/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.special import expit  # Logistic function\n",
    "from rank_metrics import ndcg_at_k, mean_average_precision\n",
    "from lightgbm import LGBMRanker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_np_arrays(train_or_val):\n",
    "    \"\"\"\n",
    "    Loads saved numpy arrays with query, doc, and relevance data\n",
    "    \n",
    "    Params:\n",
    "        train_or_val (string): enter 'train' or 'val' to load training or validation arrays\n",
    "    \n",
    "    Returns:\n",
    "        dids (np array): document ids\n",
    "        qids (np array): query ids\n",
    "        features (np array): training features\n",
    "        gold_weights (np array): labels for each document per query\n",
    "    \"\"\"\n",
    "    \n",
    "    if train_or_val == 'train':\n",
    "        folder = 'Yahoo_Numpy//Train//'\n",
    "    elif train_or_val == 'val':\n",
    "        folder = 'Yahoo_Numpy//Val//'\n",
    "    \n",
    "    dids = np.load(folder + 'dids.npy', allow_pickle=True)\n",
    "    qids = np.load(folder + 'qids.npy', allow_pickle=True)\n",
    "    features = np.load(folder + 'features.npy', allow_pickle=True)\n",
    "    gold_weights = np.load(folder + 'gold_weights.npy', allow_pickle=True)\n",
    "\n",
    "    return dids, qids, features, gold_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dids, qids, features, gold_weights, sample=True, size=10, seed=1):\n",
    "    \"\"\"\n",
    "    Samples and formats data before feature generation\n",
    "    \n",
    "    Params:\n",
    "        dids, qids, features, gold_weights (np array): see load_np_arrays\n",
    "        sample (bool): whether to sample a set of queries (True) or use all queries (False)\n",
    "        size (int): size of sample\n",
    "        seed (int): random seed for sampling, allows reproducible results\n",
    "\n",
    "    Returns:\n",
    "        q_choice (np array): unique sampled query ids \n",
    "        q_rel (np array): relevant non-unique sampled query ids, aligned row-wise w/ features\n",
    "        feat_rel (np array): relevant sampled features\n",
    "        label_rel (np array): relevant sampled labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Random seed for query sampling\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Randomly select queries\n",
    "    if sample:\n",
    "        q_choice = np.random.choice(qids, size=size, replace=False)\n",
    "    else:\n",
    "        q_choice = qids\n",
    "\n",
    "    # Get query id aligned with features\n",
    "    query_id = np.array([int(ele.split(\"_\")[1]) for ele in dids])\n",
    "    \n",
    "    # Get relevant queries, features, and labels\n",
    "    q_rel = query_id[np.isin(query_id, q_choice)]\n",
    "    feat_rel = features[np.isin(query_id, q_choice)]\n",
    "    label_rel = gold_weights[np.isin(qids, q_choice)]\n",
    "\n",
    "    # Join subarrays\n",
    "    label_rel = np.concatenate(label_rel)\n",
    "\n",
    "    # Include query id in features\n",
    "    feat_rel = np.hstack((q_rel.reshape(-1, 1), feat_rel))\n",
    "    \n",
    "    return q_choice, q_rel, feat_rel, label_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(q_choice, q_rel, feat_rel, label_rel, \n",
    "                      repeat_importance, delta_features):\n",
    "    \"\"\"\n",
    "    Generate pairwise features between documents in all queries\n",
    "    \n",
    "    Params:\n",
    "        q_choice, q_rel, feat_rel, label_rel (np array): see prepare_data\n",
    "        repeat_importance (bool): whether to duplicate rows based on the magnitude of the\n",
    "                                  difference in scores\n",
    "        delta_features (bool): whether to include the difference between document features\n",
    "                               in the pairwise generated features\n",
    "    \n",
    "    Returns:\n",
    "        features (np array): pairwise generated features, including query id (index 0) and\n",
    "                             difference in scores (index -1)\n",
    "        q_rel (np array): relevant query ids aligned with label_rel, used with label_rel in \n",
    "                          metrics\n",
    "        label_rel (np array): relevant labels aligned with q_rel, used in metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    n_rows = 0\n",
    "    max_diff = 4\n",
    "    n_features = 700\n",
    "\n",
    "    # Find max possible number of rows: n_queries * (n_urls_per_query ^ 2) * max_repeat_factor\n",
    "    for qid in q_choice:\n",
    "        urls_per_query = np.sum(np.isin(q_rel, qid))\n",
    "\n",
    "        # If not repeating importance, then every query-URL pair only appears once\n",
    "        if repeat_importance:\n",
    "            n_rows += (urls_per_query ** 2) * max_diff\n",
    "        else:\n",
    "            n_rows += (urls_per_query ** 2)\n",
    "\n",
    "    # Add extra set of columns if delta_features, + 2 for (query_id, label)\n",
    "    if delta_features:\n",
    "        n_columns = (n_features * 3) + 2\n",
    "    else:\n",
    "        n_columns = (n_features * 2) + 2\n",
    "\n",
    "    # Create array to fill in later (faster), step thru with idx\n",
    "    features = np.full(shape=(n_rows, n_columns), fill_value=np.nan)\n",
    "    idx = 0\n",
    "\n",
    "    # Iter thru queries\n",
    "    for progress, qid in enumerate(q_choice):\n",
    "\n",
    "        temp_feat = feat_rel[np.isin(q_rel, qid)]\n",
    "        temp_label = label_rel[np.isin(q_rel, qid)]\n",
    "\n",
    "        m = temp_feat.shape[0]\n",
    "\n",
    "        # First URL\n",
    "        for i in range(m):\n",
    "\n",
    "            # Second URL\n",
    "            for j in range(m):\n",
    "\n",
    "                label_diff = temp_label[i] - temp_label[j]\n",
    "\n",
    "                # Repeat importance: duplicate row |label_diff| times\n",
    "                if repeat_importance:\n",
    "                    end_k = int(abs(label_diff)) + 1\n",
    "                else:\n",
    "                    end_k = 1\n",
    "\n",
    "                for k in range(end_k):\n",
    "\n",
    "                    # Delta features: for feature (a, b), represent as (a, b, a-b)\n",
    "                    # Format: (qid, feat[i], feat[j], feat[i] - feat[j], label_diff)\n",
    "                    if delta_features:\n",
    "                        new_row = np.hstack((temp_feat[i], \n",
    "                                             temp_feat[j, 1:], \n",
    "                                             temp_feat[i, 1:] - temp_feat[j, 1:],\n",
    "                                             label_diff))\n",
    "                    else:\n",
    "                        new_row = np.hstack((temp_feat[i], \n",
    "                                             temp_feat[j, 1:], \n",
    "                                             label_diff))\n",
    "\n",
    "                    features[idx] = new_row\n",
    "                    idx += 1\n",
    "\n",
    "        print(progress + 1)\n",
    "\n",
    "    # Originally allocated array is likely too large, only save relevant rows\n",
    "    features = features[~np.isnan(features[:, 0])]\n",
    "    \n",
    "    # Also return relevant query ids and labels in order to later use metrics (MAP, NDCG)\n",
    "    return features, q_rel, label_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train_feat, test_feat, q_rel, label_rel):\n",
    "    \"\"\"\n",
    "    Trains and tests xgboost model on generated features\n",
    "    \n",
    "    Params:\n",
    "        train_feat (np array): training features\n",
    "        test_feat (np array): testing features (from validation set)\n",
    "        q_rel, label_rel (np array): see generate_features\n",
    "        \n",
    "    Returns:\n",
    "        r_ls (list of np arrays): each subarray is an ordered ranking of the original labels\n",
    "                                  for a single test query, generated by the xgboost model\n",
    "    \"\"\"\n",
    "    \n",
    "    ################################# TRAINING #################################\n",
    "    X_train = train_feat[:, :-1]\n",
    "    y_train = train_feat[:, -1]\n",
    "\n",
    "    # Same parameters for all calls to ensure consistency\n",
    "    xgbr = XGBRegressor(max_depth=6, \n",
    "                        learning_rate=0.1,\n",
    "                        n_estimators=10,\n",
    "                        objective='reg:squarederror')\n",
    "\n",
    "    xgbr.fit(X_train, y_train)\n",
    "\n",
    "    print('Model fitted')\n",
    "\n",
    "    ################################# TESTING #################################\n",
    "    # Want to make predictions on every URL pair within a query, for all queries\n",
    "    X_test = test_feat[:, :-1]\n",
    "    y_test = test_feat[:, -1]\n",
    "    y_pred = xgbr.predict(X_test)\n",
    "\n",
    "    # Query ids in test set\n",
    "    qids = np.unique(X_test[:, 0])\n",
    "    size = qids.size\n",
    "    \n",
    "    # Record results over all queries\n",
    "    MAP = 0\n",
    "    NDCG1, NDCG3, NDCG5, NDCG10, NDCGM = 0, 0, 0, 0, 0\n",
    "\n",
    "    # Save rankings (to visually compare)\n",
    "    r_ls = []\n",
    "\n",
    "    # For each query, make a prediction array (scores)\n",
    "    for qid in qids:\n",
    "\n",
    "        # m will be the number of URLs per given query ID\n",
    "        m = int(np.sqrt(np.sum(X_test[:, 0] == qid)))\n",
    "\n",
    "        # Save y_pred only for query of interest as y_pq, reshape in order to sum across rows\n",
    "        # Note that the default order='C' in reshape is fine (row-major)\n",
    "        # Setting order='F' will result in roughly the same result, just reversed since the \n",
    "        # learned labels correspond to (URLi - URLj)\n",
    "        y_pq = y_pred[X_test[:, 0] == qid]\n",
    "        y_pq = y_pq.reshape(m, m, order='C')\n",
    "\n",
    "        # Apply logistic function\n",
    "        y_pq = expit(y_pq)\n",
    "\n",
    "        # Sum across rows to get 'power' of each individual training example\n",
    "        # Get order using the scores as indices\n",
    "        scores = np.sum(y_pq, axis=0)\n",
    "        order = np.argsort(scores)\n",
    "\n",
    "        # Apply order to original labels\n",
    "        y_orig = label_rel[q_rel == qid]\n",
    "        r = y_orig[order]\n",
    "\n",
    "        # Save ranking\n",
    "        r_ls.append(r)\n",
    "\n",
    "        # Get results\n",
    "        m_a_p = mean_average_precision([r])\n",
    "        n1, n3, n5, n10, nm = (ndcg_at_k(r=r, k=1),ndcg_at_k(r=r, k=3), ndcg_at_k(r=r, k=5), \n",
    "                               ndcg_at_k(r=r, k=10), ndcg_at_k(r=r, k=m))\n",
    "\n",
    "        # Update overall results\n",
    "        MAP += m_a_p\n",
    "        NDCG1 += n1\n",
    "        NDCG3 += n3\n",
    "        NDCG5 += n5\n",
    "        NDCG10 += n10\n",
    "        NDCGM += nm\n",
    "\n",
    "        # Results for query\n",
    "        print('Query %d, m=%d:' % (qid, m))\n",
    "        print('\\tMAP:     %.4f' % m_a_p)\n",
    "        print('\\tNDCG@1:  %.4f' % n1)\n",
    "        print('\\tNDCG@3:  %.4f' % n3)\n",
    "        print('\\tNDCG@5:  %.4f' % n5)\n",
    "        print('\\tNDCG@10: %.4f' % n10)\n",
    "        print('\\tNDCG@m:  %.4f' % nm)\n",
    "\n",
    "    # Results over all queries\n",
    "    print('\\nOverall:')\n",
    "    print('\\tMAP:     %.4f' % (MAP / size))\n",
    "    print('\\tNDCG@1:  %.4f' % (NDCG1 / size))\n",
    "    print('\\tNDCG@3:  %.4f' % (NDCG3 / size))\n",
    "    print('\\tNDCG@5:  %.4f' % (NDCG5 / size))\n",
    "    print('\\tNDCG@10: %.4f' % (NDCG10 / size))\n",
    "    print('\\tNDCG@m:  %.4f' % (NDCGM / size))\n",
    "    \n",
    "    # Return list of ranking lists for each query in order to manually inspect rankings\n",
    "    return r_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambdaMART_predictions(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    # Counts of docs/query, 'group' parameter in fit()\n",
    "    _, counts = np.unique(X_train[:, 0], return_counts=True)\n",
    "    \n",
    "    # Create and fit lightGBM with same parameters as XGBoost\n",
    "    lgbm = LGBMRanker(num_leaves=127, \n",
    "                      max_depth=6,\n",
    "                      learning_rate=0.1,\n",
    "                      n_estimators=10,\n",
    "                      objective='lambdarank')\n",
    "\n",
    "    lgbm.fit(X_train, y_train, group=counts)\n",
    "    \n",
    "    # Record results over all queries\n",
    "    MAP = 0\n",
    "    NDCG1, NDCG3, NDCG5, NDCG10, NDCGM = 0, 0, 0, 0, 0\n",
    "    r_ls = []\n",
    "    \n",
    "    # Number of queries in test set\n",
    "    qids = np.unique(X_test[:, 0])\n",
    "    size = qids.size\n",
    "    \n",
    "    # Make predictions and give results for each query\n",
    "    for qid in qids:\n",
    "        \n",
    "        # Get relevant features, labels, size\n",
    "        X_pq = X_test[X_test[:, 0] == qid]\n",
    "        y_pq = y_test[X_test[:, 0] == qid]\n",
    "        m = y_pq.size\n",
    "        \n",
    "        # Make predictions and sort docs by predicted scores\n",
    "        y_pred = lgbm.predict(X_pq)\n",
    "        order = np.argsort(y_pred)[::-1]\n",
    "        r = y_pq[order]\n",
    "        r_ls.append(r)\n",
    "        \n",
    "        # Compute results\n",
    "        m_a_p = mean_average_precision([r])\n",
    "        n1, n3, n5, n10, nm = (ndcg_at_k(r=r, k=1),ndcg_at_k(r=r, k=3), ndcg_at_k(r=r, k=5), \n",
    "                               ndcg_at_k(r=r, k=10), ndcg_at_k(r=r, k=m))\n",
    "        \n",
    "        # Update overall results\n",
    "        MAP += m_a_p\n",
    "        NDCG1 += n1\n",
    "        NDCG3 += n3\n",
    "        NDCG5 += n5\n",
    "        NDCG10 += n10\n",
    "        NDCGM += nm\n",
    "        \n",
    "        # Results for query\n",
    "        print('Query %d, m=%d:' % (qid, m))\n",
    "        print('\\tMAP:     %.4f' % m_a_p)\n",
    "        print('\\tNDCG@1:  %.4f' % n1)\n",
    "        print('\\tNDCG@3:  %.4f' % n3)\n",
    "        print('\\tNDCG@5:  %.4f' % n5)\n",
    "        print('\\tNDCG@10: %.4f' % n10)\n",
    "        print('\\tNDCG@m:  %.4f' % nm)\n",
    "        \n",
    "    # Results over all queries\n",
    "    print('\\nOverall:')\n",
    "    print('\\tMAP:     %.4f' % (MAP / size))\n",
    "    print('\\tNDCG@1:  %.4f' % (NDCG1 / size))\n",
    "    print('\\tNDCG@3:  %.4f' % (NDCG3 / size))\n",
    "    print('\\tNDCG@5:  %.4f' % (NDCG5 / size))\n",
    "    print('\\tNDCG@10: %.4f' % (NDCG10 / size))\n",
    "    print('\\tNDCG@m:  %.4f' % (NDCGM / size))\n",
    "    \n",
    "    return r_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing DeltaMART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "Generated training features\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "Generated validation features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and generate features from queries in training set \n",
    "arrays = load_np_arrays('train')\n",
    "data = prepare_data(*arrays, sample=True, size=1000, seed=55)\n",
    "train_feat, _, _ = generate_features(*data, repeat_importance=True, delta_features=False)\n",
    "print('Generated training features\\n')\n",
    "del arrays, data\n",
    "\n",
    "# Load and generate features from queries in validation set\n",
    "arrays = load_np_arrays('val')\n",
    "data = prepare_data(*arrays, sample=True, size=500, seed=66)\n",
    "test_feat, q_rel, label_rel = generate_features(*data, repeat_importance=False, \n",
    "                                                delta_features=False)\n",
    "print('Generated validation features\\n')\n",
    "del arrays, data\n",
    "\n",
    "# Train/evaluate xgboost model\n",
    "# build_model(train_feat, test_feat, q_rel, label_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeltaMART Results\n",
    "\n",
    "    10 trees in XGBoost\n",
    "    100/50 queries in train/test, seed=55/66 for train/validation\n",
    "    \n",
    "    delta_features = True\n",
    "    Overall:\n",
    "        MAP:     0.8259\n",
    "        NDCG@1:  0.7567\n",
    "        NDCG@3:  0.7229\n",
    "        NDCG@5:  0.7339\n",
    "        NDCG@10: 0.7629\n",
    "        NDCG@m:  0.8450\n",
    "    \n",
    "    delta_features = False\n",
    "    Overall:\n",
    "        MAP:     0.8325\n",
    "        NDCG@1:  0.6700\n",
    "        NDCG@3:  0.7192\n",
    "        NDCG@5:  0.7148\n",
    "        NDCG@10: 0.7577\n",
    "        NDCG@m:  0.8427\n",
    "        \n",
    "        \n",
    "### LambdaMART Results\n",
    "\n",
    "    10 trees in lightGBM\n",
    "    100/50 queries in train/test, seed=55/66 for train/validation\n",
    "    \n",
    "    Overall:\n",
    "        MAP:     0.8241\n",
    "        NDCG@1:  0.6800\n",
    "        NDCG@3:  0.6773\n",
    "        NDCG@5:  0.7004\n",
    "        NDCG@10: 0.7420\n",
    "        NDCG@m:  0.8304"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LambdaMART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = load_np_arrays('train')\n",
    "_, _, X_train, y_train = prepare_data(*arrays, sample=True, size=100, seed=55)\n",
    "\n",
    "arrays = load_np_arrays('val')\n",
    "_, _, X_test, y_test = prepare_data(*arrays, sample=True, size=50, seed=66)\n",
    "\n",
    "print('Prepared data')\n",
    "\n",
    "lambdaMART_predictions(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff/experimentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdaMART_predictions(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = load_np_arrays('train')\n",
    "q_choice, q_rel, feat_rel, label_rel = prepare_data(*arrays, sample=True, size=10, seed=7)\n",
    "\n",
    "X_train = feat_rel\n",
    "y_train = label_rel\n",
    "\n",
    "feat_rel.shape, label_rel.shape, q_rel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, counts = np.unique(q_rel, return_counts=True)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lightgbm.LGBMRanker()\n",
    "lgbm.fit(feat_rel, label_rel, group=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = load_np_arrays('val')\n",
    "q_choice, q_rel, feat_rel, label_rel = prepare_data(*arrays, sample=True, size=5, seed=7)\n",
    "\n",
    "X_test = feat_rel\n",
    "y_test = label_rel\n",
    "\n",
    "feat_rel.shape, label_rel.shape, q_rel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_rel[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgbm.predict(feat_rel[feat_rel[:, 0] == np.unique(feat_rel[:, 0])[0]])\n",
    "order = np.argsort(y_pred)\n",
    "r = label_rel[order]\n",
    "\n",
    "mean_average_precision([r]), ndcg_at_k(r, k=5), r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
